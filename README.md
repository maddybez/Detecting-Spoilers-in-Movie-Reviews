This was a group project for CS3244 Machine Learning in NUS.
The whole project is not here.

<p> <b> <u> Scope of Project: </u> </b> </p> </u>

<p> Viewership experience often ruined or diminished by spoilers. Spoilers detract from the thrill and genuine emotional investment from an audience, which may often result in viewers not wanting to tune into spoiled movies, leading to revenue loss for the film industry as well. Having a model that detects movie spoilers in text all across the internet might be able to help preserve this emotional investment and relationship between the viewers, the movie and film-makers. </p>

<p> Dataset </p>

<p> IMDb Movie Reviews Dataset </p>

<p> Data Understanding (Exploratory Data Analysis) </p>

<p>- Do certain phrases contribute to a spoiler tag? </p>
<p>- Do certain users (reviewers) post spoilers more frequently? </p>
<p>- Correlation between length of review and spoiler classification?</p>

<p> Word Embeddings </p>

<p> Glove </p>

<p> Models </p>

<p> Linear models : SVM, Naive Bayes, Logistic Regression </p>
<p> Neural Networks : Convolutional Neural Network (CNN), Long Short-Term Memory Network (LSTM) </p>

<p> Model Evaluation </p>


<p> Metric Choice </p>
<p> In the context of this project, a false negative is more harmful. Allowing spoilers to fall through the net means viewers are more likely to read them and diminish their viewership </p>

<p> <b> Learning Reflection </b> </p>
<p> 1. Dealing with Imbalanced Datasets : </p>
<p> This was the first project where I had to handle a dataset with imbalance data. Using resampling methods, such as oversampling and undersampling, 
it was interesting to find out how to deploy such methods. </p>

<p> 2. Metric Choice </p>
<p> Learning to choose appropriate metrics for evaluation in context of the project. </p>

<!---
maddybez/maddybez is a ✨ special ✨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
